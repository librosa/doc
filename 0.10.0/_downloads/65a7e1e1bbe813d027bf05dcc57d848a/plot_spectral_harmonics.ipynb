{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Harmonic spectrum\n\nThis notebook demonstrates how to extract the harmonic spectrum from an audio signal.\nThe basic idea is to estimate the fundamental frequency (f0) at each time step,\nand extract the energy at integer multiples of f0 (the *harmonics*).\nThis representation can be used to represent the short-term evolution of timbre,\neither for resynthesis [1]_ or downstream analysis [2]_.\n\n.. [1] Bonada, Jordi, X. Serra, X. Amatriain, and A. Loscos.\n    \"Spectral processing.\"\n    DAFX Digital Audio Effects (2011): 393-444.\n\n.. [2] Rafii, Zafar.\n    \"The Constant-Q Harmonic Coefficients: A timbre feature designed for music signals.\"\n    IEEE Signal Processing Magazine 39, no. 3 (2022): 90-96.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Code source: Brian McFee\n# License: ISC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll need numpy and matplotlib as usual\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\n# For synthesis, we'll use mir_eval's sonify module\nimport mir_eval.sonify\n\n# For audio playback, we'll use IPython.display's Audio widget\nfrom IPython.display import Audio\n\nimport librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll start by loading a speech example to analyze\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y, sr = librosa.load(librosa.ex('libri2'), duration=5)\n\nAudio(data=y, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll estimate the fundamental frequency (f0)\nof the voice using `librosa.pyin`.\n\nWe'll constrain `f0` to lie within the range 50 Hz\nto 300 Hz.\n\npyin returns three arrays:\n  - `f0`, the sequence of fundamental frequency estimates\n  - `voicing`, the sequence of indicator variables for whether\n    a fundamental was detected or not at each time step\n  - `voicing_probability`, the sequence of probabilities that each\n    time step contains a fundamental frequency\n\nFor this application, we'll only be using `f0`.  Note that by default,\npyin will set `f0[n] = np.nan` (not a number) whenever `voicing[n] == False`.\nWe'll handle this situation later on when resynthesizing the signal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f0, voicing, voicing_probability = librosa.pyin(y=y, sr=sr, fmin=50, fmax=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize the f0 contour on top of a spectrogram as follows\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "S = np.abs(librosa.stft(y))\n\ntimes = librosa.times_like(S, sr=sr)\n\nfig, ax = plt.subplots()\nlibrosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),\n                         y_axis='log', x_axis='time', ax=ax)\nax.plot(times, f0, linewidth=2, color='white', label='f0')\nax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The figure above illustrates how the f0 contour tends to\nfollow the lowest frequency with the most energy, which\nare indicated by bright colors toward the bottom of the\nimage.  The patterns replicate at higher frequencies\ncorresponding to harmonics of the fundamental, which are\nidentified by `2*f0`, `3*f0`, `4*f0`, etc.\n\nWe can use `librosa.f0_harmonics` to extract the energy\nfrom a specified set of harmonics relative to the f0.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Let's use the first 30 harmonics: 1, 2, 3, ..., 30\nharmonics = np.arange(1, 31)\n\n# And standard Fourier transform frequencies\nfrequencies = librosa.fft_frequencies(sr=sr)\n\nharmonic_energy = librosa.f0_harmonics(S, f0=f0, harmonics=harmonics, freqs=frequencies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize the result `harmonic_energy` alongside the\nfull spectrogram `S` to see how they compare to each other.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 2\n\nfig, ax = plt.subplots(nrows=2, sharex=True)\n\nlibrosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max),\n                         y_axis='log', x_axis='time', ax=ax[0])\nlibrosa.display.specshow(librosa.amplitude_to_db(harmonic_energy, ref=np.max),\n                         x_axis='time', ax=ax[1])\nax[0].label_outer()\nax[1].set(ylabel='Harmonics')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above figure, we can observe the same general\npatterns of spectral energy as in the full spectrogram,\nbut notice that the shapes no longer move up and down\nwith the f0 contour.  The resulting `harmonic_energy`\nhas been normalized with regard to the fundamental.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the f0 contour and the harmonic energy measurements,\nwe can reconstruct an approximation to the original signal\nby sinusoidal modeling.  This really just means adding up\nsine waves at the chosen set of frequencies and scaled\nappropriately by the amount of energy at that frequency\nover time.\n\nSince the f0 contour is a time-varying frequency measurement,\nthe synthesis will need to support variable frequencies.\nLuckily, the `mir_eval.sonify` module does exactly this.\nWe'll generate a separate signal for each harmonic separately,\nand add them into a mixture to better approximate the original\nsignal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# f0 takes value np.nan for unvoiced regions, but this isn't\n# helpful for synthesis.  We'll use `np.nan_to_num` to replace\n# nans with a frequency of 0.\nf0_synth = np.nan_to_num(f0)\n\ny_out = np.zeros_like(y)\n\nfor i, (factor, energy) in enumerate(zip(harmonics, harmonic_energy)):\n    # Mix in a synthesized pitch contour\n    y_out = y_out + mir_eval.sonify.pitch_contour(times, f0_synth * factor,\n                                                amplitudes=energy,\n                                                fs=sr,\n                                                length=len(y))\n\nAudio(data=y_out, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The synthesized audio is not a perfect reconstruction\nof the input signal.  Notably, it is derived from a\nsparse sinusoidal modeling assumption, and will\ntherefore not do well at representing transients.\nThe result is still largely intelligible, however,\nand the decoupling of f0 from harmonic energy allows\nus to modify the synthesized signal in various ways.\n\nFor example, we can synthesize the same utterance\nwith a constant f0 to produce a monotone effect.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Make a fake f0 contour\nf_mono = 110 * np.ones_like(f0)\n\nymono = np.zeros_like(y)\n\nfor i, (factor, energy) in enumerate(zip(harmonics, harmonic_energy)):\n    # Use f_mono here instead of f0\n    ymono = ymono + mir_eval.sonify.pitch_contour(times, f_mono * factor,\n                                                amplitudes=energy,\n                                                fs=sr,\n                                                length=len(y))\n\nAudio(data=ymono, rate=sr)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}